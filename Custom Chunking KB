import os
import json
import uuid
import logging
import io
import re

import boto3
import PyPDF2

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS clients
s3_client = boto3.client("s3")
bedrock_client = boto3.client("bedrock-agent")

KB_ID = os.environ["BEDROCK_KB_ID"]

CHUNK_SIZE = int(os.environ.get("CHUNK_SIZE", 1000))
CHUNK_OVERLAP = int(os.environ.get("CHUNK_OVERLAP", 200))


def extract_population_id(key: str) -> str:
    """
    Extracts the population ID from the S3 key,
    assuming format: population_id/<updates|master>/filename.pdf
    """
    parts = key.split("/", 2)
    if len(parts) < 3:
        raise ValueError(f"Invalid S3 key format: '{key}'")
    return parts[0]


def extract_nin(filename: str) -> int:
    """
    Extracts the last numeric sequence in the filename as the NIN priority.
    """
    name = os.path.splitext(filename)[0]
    nums = [int(n) for n in re.findall(r"(\d+)", name)]
    if not nums:
        raise ValueError(f"No numeric sequence found in filename: '{filename}'")
    return nums[-1]


def pdf_to_text(bucket: str, key: str) -> str:
    """
    Download PDF from S3 and extract text from all pages.
    Uses an in-memory BytesIO buffer for compatibility.
    """
    obj = s3_client.get_object(Bucket=bucket, Key=key)
    pdf_bytes = obj["Body"].read()
    reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
    text_pages = []
    for page in reader.pages:
        text_pages.append(page.extract_text() or "")
    return "\n".join(text_pages)


def chunk_text(text: str):
    """
    Generator that yields overlapping text chunks.
    """
    start = 0
    length = len(text)
    while start < length:
        end = min(length, start + CHUNK_SIZE)
        yield text[start:end]
        start += CHUNK_SIZE - CHUNK_OVERLAP


def lambda_handler(event, context):
    """
    S3:ObjectCreated-triggered Lambda.
    Reads a PDF, chunks it, tags with metadata, and ingests into a Bedrock KB.
    """
    try:
        record = event["Records"][0]
        bucket = record["s3"]["bucket"]["name"]
        key = record["s3"]["object"]["key"]
        filename = key.split("/")[-1]

        logger.info(f"Processing s3://{bucket}/{key}")

        # Extract metadata
        population_id = extract_population_id(key)
        nin_priority = extract_nin(filename)
        logger.info(f"Population: {population_id}, NIN priority: {nin_priority}")

        # Extract text and chunk
        text = pdf_to_text(bucket, key)
        documents = []
        for chunk in chunk_text(text):
            documents.append({
                "content": chunk,
                "metadata": {
                    'inlineAttributes': [
                        "population_id": population_id,
                        "nin_priority": nin_priority,
                        "source_key": key
                    ]
                    
                }
            })

        # Ingest into Bedrock Knowledge Base
        response = bedrock_client.ingest_knowledge_base_documents(
            knowledgeBaseId=KB_ID,
            documents=documents
        )

        job_id = response.get("jobId")
        logger.info(f"Ingested {len(documents)} chunks, jobId={job_id}")

        return {
            "statusCode": 200,
            "body": json.dumps({
                "message": f"Ingested {len(documents)} chunks from {filename}",
                "jobId": job_id
            })
        }

    except Exception as e:
        logger.error(f"Error processing S3 record: {e}", exc_info=True)
        return {
            "statusCode": 500,
            "body": json.dumps({"error": str(e)})
        }
