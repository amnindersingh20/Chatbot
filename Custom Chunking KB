import os
import re
import json
import uuid
import logging

import boto3
import PyPDF2

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS clients
s3_client = boto3.client("s3")
bedrock_client = boto3.client(
    "bedrock", region_name=os.environ.get("AWS_REGION", "us-east-1")
)

# Environment variables
KB_ID = os.environ["BEDROCK_KB_ID"]           # The Bedrock Knowledge Base ARN or ID
CHUNK_SIZE = int(os.environ.get("CHUNK_SIZE", 1000))
CHUNK_OVERLAP = int(os.environ.get("CHUNK_OVERLAP", 200))

# Regex to extract the NIN (numeric identifier) from filename
NIN_RE = re.compile(r"^[^-]+-[^-]+-(\d+)-?.*\.pdf$", re.IGNORECASE)

# Handler

def extract_population_id(key: str) -> str:
    """
    Extracts the population ID from the S3 key, assuming the key format:
      population_id/<updates|master>/filename.pdf
    """
    parts = key.split('/', 2)
    if len(parts) < 3:
        raise ValueError(f"Invalid S3 key format for population extraction: '{key}'")
    return parts[0]


def extract_nin(filename: str) -> int:
    """
    Extract numeric NIN from filename of form: prefix1-prefix2-NIN-... .pdf
    """
    match = NIN_RE.match(filename)
    if not match:
        raise ValueError(f"Filename does not match expected pattern for NIN: '{filename}'")
    return int(match.group(1))


def pdf_to_text(bucket: str, key: str) -> str:
    """
    Download PDF from S3 and extract text from all pages.
    """
    obj = s3_client.get_object(Bucket=bucket, Key=key)
    reader = PyPDF2.PdfReader(obj["Body"])
    text_parts = []
    for page in reader.pages:
        text_parts.append(page.extract_text() or "")
    return "\n".join(text_parts)


def chunk_text(text: str):
    """
    Generator that yields overlapping text chunks.
    """
    start = 0
    length = len(text)
    while start < length:
        end = min(length, start + CHUNK_SIZE)
        yield text[start:end]
        start += CHUNK_SIZE - CHUNK_OVERLAP


def handler(event, context):
    """
    Lambda handler triggered by S3:ObjectCreated. Ingests PDF chunks into Bedrock KB
    with metadata: population_id and nin_priority.
    """
    try:
        record = event['Records'][0]
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        filename = key.split('/')[-1]

        logger.info(f"Processing file: s3://{bucket}/{key}")

        # Extract metadata
        population_id = extract_population_id(key)
        nin_priority = extract_nin(filename)
        logger.info(f"Population: {population_id}, NIN: {nin_priority}")

        # Extract text
        text = pdf_to_text(bucket, key)

        # Chunk text and prepare documents
        documents = []
        for chunk in chunk_text(text):
            doc = {
                'id': str(uuid.uuid4()),
                'content': chunk,
                'metadata': {
                    'population_id': population_id,
                    'nin_priority': nin_priority,
                    'source_key': key
                }
            }
            documents.append(doc)

        # Ingest into Bedrock KB
        response = bedrock_client.ingest_knowledge_base_documents(
            knowledgeBaseId=KB_ID,
            documents=documents
        )

        logger.info(f"Ingested {len(documents)} chunks, job ID: {response.get('jobId')}")

        return {
            'statusCode': 200,
            'body': json.dumps({
                'message': f"Ingested {len(documents)} chunks from {filename}",
                'jobId': response.get('jobId')
            })
        }

    except Exception as e:
        logger.error(f"Error processing record: {e}", exc_info=True)
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }
