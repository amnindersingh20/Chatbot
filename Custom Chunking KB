import os
import json
import uuid
import logging
import io
import re

import boto3
import PyPDF2

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS clients
s3_client = boto3.client("s3")
agent_client = boto3.client(
    "bedrock-agent",
    region_name=os.environ.get("AWS_REGION", "us-east-1")
)

# Environment variables (set these in the Lambda Console)
KB_ID          = os.environ["BEDROCK_KB_ID"]       # e.g. arn:aws:bedrock:...:knowledgebase/kb-abc123
DATA_SOURCE_ID = os.environ["DATA_SOURCE_ID"]      # S3 data source ID in that KB

# Chunking parameters (optional overrides)
CHUNK_SIZE    = int(os.environ.get("CHUNK_SIZE", 1000))
CHUNK_OVERLAP = int(os.environ.get("CHUNK_OVERLAP", 200))


def extract_population_id(key: str) -> str:
    """
    Extracts the population ID from the S3 key,
    assuming format: population_id/<updates|master>/filename.pdf
    """
    parts = key.split("/", 2)
    if len(parts) < 3:
        raise ValueError(f"Invalid S3 key format: '{key}'")
    return parts[0]


def extract_nin(filename: str) -> int:
    """
    Extracts the last numeric sequence in the filename as the NIN priority.
    """
    name = os.path.splitext(filename)[0]
    nums = [int(n) for n in re.findall(r"(\d+)", name)]
    if not nums:
        raise ValueError(f"No numeric sequence found in filename: '{filename}'")
    return nums[-1]


def pdf_to_text(bucket: str, key: str) -> str:
    """
    Download PDF from S3 and extract text from all pages
    using an in-memory buffer for PyPDF2.
    """
    obj = s3_client.get_object(Bucket=bucket, Key=key)
    pdf_bytes = obj["Body"].read()
    reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
    pages = [page.extract_text() or "" for page in reader.pages]
    return "\n".join(pages)


def chunk_text(text: str):
    """
    Generator that yields overlapping text chunks.
    """
    start = 0
    length = len(text)
    while start < length:
        end = min(length, start + CHUNK_SIZE)
        yield text[start:end]
        start += CHUNK_SIZE - CHUNK_OVERLAP


def lambda_handler(event, context):
    """
    S3:ObjectCreated-triggered Lambda.
    Reads a PDF, chunks it, tags chunks with inlineAttributes,
    and ingests into a Bedrock Knowledge Base.
    """
    try:
        # Parse S3 event
        record   = event["Records"][0]
        bucket   = record["s3"]["bucket"]["name"]
        key      = record["s3"]["object"]["key"]
        filename = key.split("/")[-1]

        logger.info(f"Processing s3://{bucket}/{key}")

        # Extract metadata
        population_id = extract_population_id(key)
        nin_priority  = extract_nin(filename)
        logger.info(f"Population={population_id}, NIN={nin_priority}")

        # Extract text and chunk it
        text = pdf_to_text(bucket, key)
        documents = []
        for chunk in chunk_text(text):
            documents.append({
                "content": chunk,
                "metadata": {
                    "type": "INLINE",
                    "inlineAttributes": [
                        {
                            "key": "population_id",
                            "type": "STRING",
                            "value": {"stringValue": population_id}
                        },
                        {
                            "key": "nin_priority",
                            "type": "NUMBER",
                            "value": {"numberValue": float(nin_priority)}
                        },
                        {
                            "key": "source_key",
                            "type": "STRING",
                            "value": {"stringValue": key}
                        }
                    ]
                }
            })

        # Ingest into Bedrock KB
        response = agent_client.ingest_knowledge_base_documents(
            knowledgeBaseId=KB_ID,
            dataSourceId=DATA_SOURCE_ID,
            documents=documents
        )

        job_id = response.get("jobId")
        logger.info(f"Ingested {len(documents)} chunks, jobId={job_id}")

        return {
            "statusCode": 200,
            "body": json.dumps({
                "message": f"Ingested {len(documents)} chunks from {filename}",
                "jobId": job_id
            })
        }

    except Exception as e:
        logger.error("Failed to ingest document", exc_info=True)
        return {
            "statusCode": 500,
            "body": json.dumps({"error": str(e)})
        }
