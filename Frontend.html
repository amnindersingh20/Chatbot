import os
import json
import re
import logging

import boto3
from botocore.exceptions import ClientError

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Initialize S3 client
s3 = boto3.client("s3")

# Environment variable: the bucket to process
BUCKET = os.environ["PDF_BUCKET"]  # e.g. "my-pdf-bucket"

# Regex: find last numeric sequence in a filename (for NIN priority)
_NUM_RE = re.compile(r"(\d+)(?!.*\d)")

def extract_population_and_type(key: str):
    """
    Given an S3 key like "populationA/updates/foo.pdf" or
    "populationB/master/bar.pdf", return (population_id, doc_type).
    """
    parts = key.split("/", 2)
    if len(parts) < 3:
        raise ValueError(f"Invalid PDF key format (expected population/type/...): '{key}'")
    population_id = parts[0]
    doc_type      = parts[1]  # "updates" or "master"
    return population_id, doc_type

def extract_nin(filename: str) -> int:
    """
    Extract the last numeric sequence in the filename as NIN priority.
    """
    base = os.path.splitext(filename)[0]
    m = _NUM_RE.search(base)
    if not m:
        raise ValueError(f"No numeric sequence found for NIN in '{filename}'")
    return int(m.group(1))

def metadata_key_for(pdf_key: str) -> str:
    """
    Return the S3 key for the side‑car metadata JSON.
    """
    return pdf_key + ".metadata.json"

def lambda_handler(event, context):
    """
    Lambda entry point. Scans the bucket and writes side‑car metadata JSON
    for each PDF that doesn’t already have one.
    """
    continuation_token = None
    processed = 0
    skipped   = 0

    while True:
        # List objects in pages of 1000
        kwargs = {"Bucket": BUCKET, "MaxKeys": 1000}
        if continuation_token:
            kwargs["ContinuationToken"] = continuation_token

        resp = s3.list_objects_v2(**kwargs)
        for obj in resp.get("Contents", []):
            key = obj["Key"]
            if not key.lower().endswith(".pdf"):
                continue  # ignore non‑PDFs

            meta_key = metadata_key_for(key)

            # Skip if metadata already exists
            try:
                s3.head_object(Bucket=BUCKET, Key=meta_key)
                logger.debug(f"Skipping {key}: metadata already exists")
                skipped += 1
                continue
            except ClientError as e:
                if e.response["Error"]["Code"] != "404":
                    logger.error(f"Error checking metadata for {key}: {e}")
                    continue
                # 404 means metadata not there → we should create it

            # Extract population, type, and NIN
            try:
                population_id, doc_type = extract_population_and_type(key)
                nin = extract_nin(key.split("/")[-1])
            except ValueError as e:
                logger.warning(f"Skipping {key}: {e}")
                skipped += 1
                continue

            # Build metadata JSON
            metadata = {
                "population_id": population_id,
                "doc_type":      doc_type,       # "updates" or "master"
                "nin_priority":  nin
            }

            # Write side‑car JSON
            try:
                s3.put_object(
                    Bucket=BUCKET,
                    Key=meta_key,
                    Body=json.dumps(metadata, indent=2).encode("utf-8"),
                    ContentType="application/json"
                )
                logger.info(f"Wrote metadata for {key} → {meta_key}")
                processed += 1
            except ClientError as e:
                logger.error(f"Failed to write metadata for {key}: {e}")
                skipped += 1

        # Handle pagination
        if not resp.get("IsTruncated"):  
            break
        continuation_token = resp.get("NextContinuationToken")

    logger.info(f"Done: processed={processed}, skipped={skipped}")
    return {
        "statusCode": 200,
        "body": json.dumps({
            "processed": processed,
            "skipped":   skipped
        })
    }
