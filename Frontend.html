import os
import json
import uuid
import logging
import io
import re

import boto3
import PyPDF2

logger = logging.getLogger()
logger.setLevel(logging.INFO)

s3 = boto3.client("s3")
bedrock = boto3.client("bedrock-agent", region_name=os.environ.get("AWS_REGION","us-east-1"))

KB_ID = os.environ["BEDROCK_KB_ID"]
DATA_SOURCE_ID = os.environ["DATA_SOURCE_ID"]

CHUNK_SIZE    = int(os.environ.get("CHUNK_SIZE", 1000))
CHUNK_OVERLAP = int(os.environ.get("CHUNK_OVERLAP", 200))

def extract_population_id(key: str) -> str:
    parts = key.split("/",2)
    if len(parts)<3:
        raise ValueError(f"Invalid S3 key format: {key}")
    return parts[0]

def extract_nin(filename: str) -> int:
    name = os.path.splitext(filename)[0]
    nums = [int(n) for n in re.findall(r"(\d+)", name)]
    if not nums:
        raise ValueError(f"No digits in filename: {filename}")
    return nums[-1]

def pdf_to_text(bucket, key):
    obj = s3.get_object(Bucket=bucket,Key=key)
    data = obj["Body"].read()
    reader = PyPDF2.PdfReader(io.BytesIO(data))
    return "\n".join(p.extract_text() or "" for p in reader.pages)

def chunk_text(text):
    start=0
    L=len(text)
    while start<L:
        end=min(L, start+CHUNK_SIZE)
        yield text[start:end]
        start += CHUNK_SIZE-CHUNK_OVERLAP

def lambda_handler(event, context):
    try:
        rec = event["Records"][0]
        bucket = rec["s3"]["bucket"]["name"]
        key = rec["s3"]["object"]["key"]
        filename = key.split("/")[-1]

        logger.info(f"Processing {bucket}/{key}")
        population_id = extract_population_id(key)
        nin = extract_nin(filename)

        full_text = pdf_to_text(bucket,key)
        docs = []
        for chunk in chunk_text(full_text):
            chunk = chunk.strip()
            if not chunk:
                continue
            doc_id = str(uuid.uuid4())
            docs.append({
              "content": {
                "custom": {
                  "customDocumentIdentifier": { "id": doc_id },
                  "inlineContent": {
                    "type":"text/plain",
                    "textContent": { "data": chunk }
                  }
                }
              },
              "metadata": {
                "type": "INLINE",
                "inlineAttributes": [
                  {"key":"population_id", "value": {"type": "STRING", "stringValue": population_id}},
                  {"key":"nin_priority",  "value": {"type": "NUMBER", "numberValue": float(nin)}},
                  {"key":"source_key",    "value": {"type": "STRING", "stringValue": key}},
                  "dataSourceType": "CUSTOM"
                ]
              }
            })

        resp = bedrock.ingest_knowledge_base_documents(
          knowledgeBaseId=KB_ID,
          dataSourceId=DATA_SOURCE_ID,
          documents=docs
        )
        logger.info("Ingestion job: %s", resp.get("jobId"))
        return {"statusCode":200, "body":json.dumps(resp)}

    except Exception as e:
        logger.error("Error", exc_info=True)
        return {"statusCode":500, "body":str(e)}

# import os
# import json
# import uuid
# import logging
# import io
# import re

# import boto3
# import PyPDF2

# # Configure logging
# logger = logging.getLogger()
# logger.setLevel(logging.INFO)

# # AWS clients
# s3_client = boto3.client("s3")
# bedrock_client = boto3.client("bedrock-agent")

# KB_ID = os.environ["BEDROCK_KB_ID"]

# CHUNK_SIZE = int(os.environ.get("CHUNK_SIZE", 1000))
# CHUNK_OVERLAP = int(os.environ.get("CHUNK_OVERLAP", 200))


# def extract_population_id(key: str) -> str:
#     """
#     Extracts the population ID from the S3 key,
#     assuming format: population_id/<updates|master>/filename.pdf
#     """
#     parts = key.split("/", 2)
#     if len(parts) < 3:
#         raise ValueError(f"Invalid S3 key format: '{key}'")
#     return parts[0]


# def extract_nin(filename: str) -> int:
#     """
#     Extracts the last numeric sequence in the filename as the NIN priority.
#     """
#     name = os.path.splitext(filename)[0]
#     nums = [int(n) for n in re.findall(r"(\d+)", name)]
#     if not nums:
#         raise ValueError(f"No numeric sequence found in filename: '{filename}'")
#     return nums[-1]


# def pdf_to_text(bucket: str, key: str) -> str:
#     """
#     Download PDF from S3 and extract text from all pages.
#     Uses an in-memory BytesIO buffer for compatibility.
#     """
#     obj = s3_client.get_object(Bucket=bucket, Key=key)
#     pdf_bytes = obj["Body"].read()
#     reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
#     text_pages = []
#     for page in reader.pages:
#         text_pages.append(page.extract_text() or "")
#     return "\n".join(text_pages)


# def chunk_text(text: str):
#     """
#     Generator that yields overlapping text chunks.
#     """
#     start = 0
#     length = len(text)
#     while start < length:
#         end = min(length, start + CHUNK_SIZE)
#         yield text[start:end]
#         start += CHUNK_SIZE - CHUNK_OVERLAP


# def lambda_handler(event, context):
#     """
#     S3:ObjectCreated-triggered Lambda.
#     Reads a PDF, chunks it, tags with metadata, and ingests into a Bedrock KB.
#     """
#     try:
#         record = event["Records"][0]
#         bucket = record["s3"]["bucket"]["name"]
#         key = record["s3"]["object"]["key"]
#         filename = key.split("/")[-1]

#         logger.info(f"Processing s3://{bucket}/{key}")

#         # Extract metadata
#         population_id = extract_population_id(key)
#         nin_priority = extract_nin(filename)
#         logger.info(f"Population: {population_id}, NIN priority: {nin_priority}")

#         # Extract text and chunk
#         text = pdf_to_text(bucket, key)
#         documents = []
#         for chunk in chunk_text(text):
#             documents.append({
#                 "content": chunk,
#                 "metadata": {
#                     'inlineAttributes': {
#                         "population_id": population_id,
#                         "nin_priority": nin_priority,
#                         "source_key": key
#                 }
                    
#                 }
#             })

#         # Ingest into Bedrock Knowledge Base
#         response = bedrock_client.ingest_knowledge_base_documents(
#             knowledgeBaseId=KB_ID,
#             documents=documents
#         )

#         job_id = response.get("jobId")
#         logger.info(f"Ingested {len(documents)} chunks, jobId={job_id}")

#         return {
#             "statusCode": 200,
#             "body": json.dumps({
#                 "message": f"Ingested {len(documents)} chunks from {filename}",
#                 "jobId": job_id
#             })
#         }

#     except Exception as e:
#         logger.error(f"Error processing S3 record: {e}", exc_info=True)
#         return {
#             "statusCode": 500,
#             "body": json.dumps({"error": str(e)})
#         }
