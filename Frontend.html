#!/usr/bin/env python3
"""
Compare a pipe-delimited CSV file of employee IDs against a target file (CSV or TXT).
Removes blocks/rows in the target file whose header/first field matches an ID in the CSV.
For TXT: blocks are delimited by lines starting with an 8-digit ID.
For CSV: rows are removed if the first field matches an ID.
Removed data is logged to a timestamped file automatically (no CLI param needed).
Automatically prints reference IDs loaded, checks numeric header (skips header row), logs target IDs (including non-numeric first row),
and writes both reference and target IDs to separate txt files.
"""
import csv
import argparse
import sys
import re
from datetime import datetime

# Timestamp for all outputs
TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')


def load_csv_ids(csv_path, delimiter='|'):
    ids = set()
    print(f"Loading reference IDs from: {csv_path}")
    with open(csv_path, newline='', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter=delimiter)
        for row in reader:
            if not row:
                continue
            emp_id = row[0].strip()
            if re.fullmatch(r"\d{8}", emp_id):
                print(f"  -> found reference ID: {emp_id}")
                ids.add(emp_id)
    print(f"Loaded {len(ids)} unique reference IDs.")
    return ids


def filter_csv_by_id(csv_in, csv_out, removed_log, id_set, delimiter=','):
    total = 0
    removed = 0
    target_ids = []

    print(f"Processing target CSV: {csv_in}")
    with open(csv_in, newline='', encoding='utf-8') as fin, \
         open(csv_out, 'w', newline='', encoding='utf-8') as fout, \
         open(removed_log, 'w', newline='', encoding='utf-8') as logf:
        reader = csv.reader(fin, delimiter=delimiter)
        writer = csv.writer(fout, delimiter=delimiter)
        log_writer = csv.writer(logf, delimiter=delimiter)

        # read first row always
        first = next(reader, None)
        if first is not None:
            total += 1
            field = first[0].strip()
            target_ids.append(field)
            print(f"First row first field: '{field}'")
            if not re.fullmatch(r"\d{8}", field) or field not in id_set:
                writer.writerow(first)
            else:
                removed += 1
                log_writer.writerow(first)

        # process rest
        for row in reader:
            total += 1
            if not row:
                continue
            field = row[0].strip()
            target_ids.append(field)
            print(f"  -> target ID: {field}")
            if re.fullmatch(r"\d{8}", field) and field in id_set:
                removed += 1
                log_writer.writerow(row)
                continue
            writer.writerow(row)

    # write target IDs
    tgt_file = f"target_ids_{TIMESTAMP}.txt"
    with open(tgt_file, 'w', encoding='utf-8') as t:
        t.write(f"# Source: {csv_in}\n")
        for tid in target_ids:
            t.write(tid + '\n')
    print(f"Wrote all target IDs to: {tgt_file}")

    print(f"Processed {total} rows; removed {removed} rows (logged to {removed_log})")


def filter_txt_by_id(txt_in, txt_out, removed_log, id_set):
    total = 0
    removed = 0
    target_ids = []
    id_re = re.compile(r"^(?P<id>\d{8})")

    print(f"Processing target TXT: {txt_in}")
    with open(txt_in, 'r', encoding='utf-8') as fin, \
         open(txt_out, 'w', encoding='utf-8') as fout, \
         open(removed_log, 'w', encoding='utf-8') as logf:
        for line in fin:
            total += 1
            m = id_re.match(line)
            if m:
                field = m.group('id')
                target_ids.append(field)
                print(f"  -> TXT block ID: {field}")
                if field in id_set:
                    removed = removed + 1
                    logf.write(line)
                    continue
            fout.write(line)

    # write target IDs
    tgt_file = f"target_ids_{TIMESTAMP}.txt"
    with open(tgt_file, 'w', encoding='utf-8') as t:
        t.write(f"# Source: {txt_in}\n")
        for tid in target_ids:
            t.write(tid + '\n')
    print(f"Wrote all target IDs to: {tgt_file}")

    print(f"Processed {total} lines; removed {removed} (logged to {removed_log})")


def main():
    parser = argparse.ArgumentParser(description="Filter by employee ID")
    parser.add_argument('reference_csv')
    parser.add_argument('target_file')
    parser.add_argument('output_file')
    parser.add_argument('log_file')
    parser.add_argument('--target_type', choices=['csv','txt'], required=True)
    parser.add_argument('--ref_delimiter', default='|')
    parser.add_argument('--target_delimiter', default=',')
    args = parser.parse_args()

    ids = load_csv_ids(args.reference_csv, delimiter=args.ref_delimiter)
    if args.target_type == 'csv':
        filter_csv_by_id(
            args.target_file,
            args.output_file,
            args.log_file,
            ids,
            delimiter=args.target_delimiter
        )
    else:
        filter_txt_by_id(
            args.target_file,
            args.output_file,
            args.log_file,
            ids
        )

if __name__ == '__main__':
    main()
