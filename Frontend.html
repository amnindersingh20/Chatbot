#!/usr/bin/env python3
"""
Compare a pipe-delimited CSV file of employee IDs against a target file (CSV or TXT).
Removes blocks/rows in the target file whose header/first field matches an ID in the CSV.
For TXT: blocks are delimited by lines starting with an 8-digit ID.
For CSV: rows are removed if the first field matches an ID.
Removed data is logged to a timestamped file automatically (no CLI param needed).
Automatically prints reference IDs loaded, checks numeric header, logs target IDs as processed,
and writes both reference and target IDs to separate txt files.
"""
import csv
import argparse
import sys
import re
from datetime import datetime

# generate a single timestamp for all output files
TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')


def load_csv_ids(csv_path, delimiter="|"):
    """
    Load the CSV and build a set of 8-digit employee IDs (first CSV field).
    Also prints each unique ID as it’s read.
    """
    ids = set()
    print(f"Loading reference IDs from: {csv_path}")
    with open(csv_path, newline='', encoding='utf-8') as f:
        reader = csv.reader(f, delimiter=delimiter)
        for row in reader:
            if not row:
                continue
            emp_id = row[0].strip()
            if re.fullmatch(r"\d{8}", emp_id):
                if emp_id not in ids:
                    print(f"  -> found reference ID: {emp_id}")
                ids.add(emp_id)
    print(f"Loaded {len(ids)} unique reference IDs.")
    return ids


def filter_txt_by_id(txt_in, txt_out, removed_log, id_set):
    """
    Read the TXT, detect blocks by 8-digit ID at line start, and remove matching blocks.
    Writes removed blocks to log and keeps other lines in output.
    """
    in_remove = False
    total_lines = 0
    removed_lines = 0
    blocks_removed = 0

    id_header_re = re.compile(r"^(?P<id>\d{8})")

    print(f"Processing target TXT: {txt_in}")
    with open(txt_in, 'r', encoding='utf-8') as fin, \
         open(txt_out, 'w', encoding='utf-8') as fout, \
         open(removed_log, 'w', encoding='utf-8') as logf:
        for line in fin:
            total_lines += 1
            m = id_header_re.match(line.strip())
            if m:
                current_id = m.group('id')
                print(f"  -> TXT block ID: {current_id}")
                if current_id in id_set:
                    in_remove = True
                    blocks_removed += 1
                else:
                    in_remove = False

            if in_remove:
                removed_lines += 1
                logf.write(line)
            else:
                fout.write(line)

    print(f"Processed {total_lines} lines from TXT: {txt_in}")
    print(f"Removed {removed_lines} lines across {blocks_removed} blocks (logged to {removed_log})")
    if removed_lines == 0:
        print("Warning: no lines removed. Check that the TXT headers use 8-digit IDs matching the reference CSV.", file=sys.stderr)


def filter_csv_by_id(csv_in, csv_out, removed_log, id_set, delimiter=","):
    """
    Read the CSV, remove rows whose first field matches an ID in id_set.
    Writes removed rows to log and keeps other rows in output.
    Skips a header row if the first field is not an 8-digit ID.
    Prints each first-field value as it’s encountered.
    Collects all target IDs (header and data) and writes them to a txt file.
    """
    total_rows = 0
    removed_rows = 0
    target_ids = []

    print(f"Processing target CSV: {csv_in}")
    with open(csv_in, newline='', encoding='utf-8') as fin, \
         open(csv_out, 'w', newline='', encoding='utf-8') as fout, \
         open(removed_log, 'w', newline='', encoding='utf-8') as logf:
        reader = csv.reader(fin, delimiter=delimiter)
        writer = csv.writer(fout, delimiter=delimiter)
        log_writer = csv.writer(logf, delimiter=delimiter)

        # Handle optional header row
        first_row = next(reader, None)
        if first_row:
            total_rows += 1
            first_field = first_row[0].strip()
            is_numeric = bool(re.fullmatch(r"\d{8}", first_field))
            print(f"First row first field ('{first_field}') is {'numeric' if is_numeric else 'non-numeric'}.")
            target_ids.append(first_field)
            if not is_numeric:
                writer.writerow(first_row)
            else:
                print(f"  -> target ID removed: {first_field}" if first_field in id_set else f"  -> target ID kept: {first_field}")
                if first_field in id_set:
                    removed_rows += 1
                    log_writer.writerow(first_row)
                else:
                    writer.writerow(first_row)

        # Process remaining rows
        for row in reader:
            total_rows += 1
            if row:
                field0 = row[0].strip()
                print(f"  -> target ID: {field0}")
                target_ids.append(field0)
                if re.fullmatch(r"\d{8}", field0) and field0 in id_set:
                    removed_rows += 1
                    log_writer.writerow(row)
                    continue
            writer.writerow(row)

    # write target IDs to file
    target_ids_file = f"target_ids_{TIMESTAMP}.txt"
    with open(target_ids_file, 'w', encoding='utf-8') as tf:
        for tid in target_ids:
            tf.write(tid + '\n')
    print(f"Wrote all target IDs to: {target_ids_file}")

    print(f"Processed {total_rows} rows from CSV: {csv_in}")
    print(f"Removed {removed_rows} rows (logged to {removed_log})")
    if removed_rows == 0:
        print("Warning: no rows removed. Check that the CSV headers or data use 8-digit IDs matching the reference CSV.", file=sys.stderr)


def main():
    parser = argparse.ArgumentParser(
        description="Filter blocks/rows by employee ID (first 8 digits), logging removed data."    )
    parser.add_argument('reference_csv', help='Path to the pipe-delimited CSV of IDs')
    parser.add_argument('target_file', help='Path to the target file (CSV or TXT)')
    parser.add_argument('output_file', help='Path for filtered output')
    parser.add_argument('--target_type', choices=['csv', 'txt'], required=True,
                        help='Type of target file to filter: "csv" or "txt"')
    parser.add_argument('--ref_delimiter', default='|', help='Delimiter for the reference CSV (default: "|")')
    parser.add_argument('--target_delimiter', default=',', help='Delimiter for the target CSV (default: ",")')
    args = parser.parse_args()

    # generate filenames
    removed_log = f"removed_data_{TIMESTAMP}.txt"
    reference_ids_file = f"reference_ids_{TIMESTAMP}.txt"

    print(f"Logging removed data to: {removed_log}")
