import json
import boto3
from langchain.chains import LLMChain
from langchain.llms.bedrock import Bedrock
from langchain.prompts import PromptTemplate
from botocore.exceptions import ClientError, BotoCoreError

# Constants
MAX_INPUT_LENGTH = 500
SUPPORTED_LANGUAGES = {'english', 'spanish'}
MODEL_ID = "anthropic.claude-v2"
REGION_NAME = "us-east-1"

# Initialize Bedrock client
def initialize_bedrock():
    try:
        return boto3.client(
            service_name='bedrock-runtime',
            region_name=REGION_NAME
        )
    except (BotoCoreError, ClientError) as error:
        raise RuntimeError(f"Bedrock initialization failed: {str(error)}") from error

# Validate input parameters
def validate_input(event):
    try:
        body = json.loads(event['body'])
        language = body.get('language', 'english').lower()
        question = body.get('question', '').strip()
        
        if not question:
            raise ValueError("Question cannot be empty")
            
        if len(question) > MAX_INPUT_LENGTH:
            raise ValueError(f"Question exceeds {MAX_INPUT_LENGTH} character limit")
            
        if language not in SUPPORTED_LANGUAGES:
            raise ValueError(f"Unsupported language: {language}")
            
        return language, question
        
    except json.JSONDecodeError:
        raise ValueError("Invalid JSON format in request body")
    except KeyError:
        raise ValueError("Missing required field in request body")

# Initialize LLM chain (cached for performance)
def get_llm_chain():
    bedrock_client = initialize_bedrock()
    
    llm = Bedrock(
        client=bedrock_client,
        model_id=MODEL_ID,
        model_kwargs={
            "max_tokens_to_sample": 1000,
            "temperature": 0.7,
            "top_p": 0.9
        }
    )
    
    prompt_template = PromptTemplate(
        input_variables=["language", "freeform_text"],
        template="You are a helpful assistant. Respond in {language}.\n\nQuestion: {freeform_text}"
    )
    
    return LLMChain(llm=llm, prompt=prompt_template)

# Main Lambda handler
def lambda_handler(event, context):
    try:
        # Validate input
        language, question = validate_input(event)
        
        # Process request
        llm_chain = get_llm_chain()
        response = llm_chain({'language': language, 'freeform_text': question})
        
        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'body': json.dumps({
                'response': response['text'],
                'model': MODEL_ID
            })
        }
        
    except ValueError as e:
        return {
            'statusCode': 400,
            'headers': {'Content-Type': 'application/json'},
            'body': json.dumps({'error': str(e)})
        }
    except RuntimeError as e:
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json'},
            'body': json.dumps({'error': str(e)})
        }
    except Exception as e:
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json'},
            'body': json.dumps({'error': 'Internal server error'})
        }
