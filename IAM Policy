import logging
import json
import boto3
import pandas as pd

# NLP imports
import spacy
from rapidfuzz import process

# Initialize logger
glog = logging.getLogger()
glog.setLevel(logging.INFO)

# AWS and file settings
S3_BUCKET = "pocbotai"
S3_KEY    = "2025 Medical HPCC Combined.csv"
FALLBACK_LAMBDA_NAME = "Poc_lda_ChabotFallback"
BEDROCK_MODEL_ID     = "anthropic.claude-3-5-sonnet-20240620-v1:0"

# AWS clients
_s3      = boto3.client('s3')
_lambda  = boto3.client('lambda')
_bedrock = boto3.client('bedrock-runtime', region_name="us-east-1")

# Load spaCy model for English (stop words, lemmatization)
nlp = spacy.load("en_core_web_sm", disable=["parser","ner"])
STOP_WORDS = nlp.Defaults.stop_words

# A small canonical map for benefits terms
CANONICAL = {
    "copayment": {"copay", "co-pay", "copayments"},
    "coinsurance": {"coinsurance", "co-insurance"},
    "out_of_pocket": {"oop", "out of pocket"},
    "deductible": {"deductible", "deductibles"},
}
# Reverse lookup alias -> canonical
REVERSE = {alias: canon for canon, aliases in CANONICAL.items() for alias in aliases}

# Load and normalize the dataframe once

def load_dataframe():
    try:
        obj = _s3.get_object(Bucket=S3_BUCKET, Key=S3_KEY)
        df = pd.read_csv(
            obj['Body'], dtype=str
        )
        df.columns = df.columns.str.strip()
        # normalize Data Point Name to lowercase + remove BOM
        df['Data Point Name'] = (
            df['Data Point Name']
            .str.replace('â€“', '-', regex=False)
            .str.replace('\u200b', '', regex=False)
            .str.strip().str.lower()
        )
        # a simple normalized key for matching
        df['normalized_name'] = df['Data Point Name']\
            .str.replace(r'[^a-z0-9_]', '', regex=True)
        return df
    except Exception:
        glog.exception("Failed to load CSV from S3")
        return pd.DataFrame()

DF = load_dataframe()

# Text preprocessing
def preprocess(text: str) -> str:
    """
    Lowercase, remove stop words & punctuation, lemmatize.
    """
    doc = nlp(text.lower())
    tokens = [
        tok.lemma_ for tok in doc
        if not tok.is_punct
           and not tok.is_space
           and tok.lemma_ not in STOP_WORDS
    ]
    return " ".join(tokens)

# Map any recognized alias to its canonical form
def normalize_concepts(text: str) -> str:
    tokens = text.split()
    normalized = [REVERSE.get(tok, tok) for tok in tokens]
    # remove residual stop words
    normalized = [tok for tok in normalized if tok not in STOP_WORDS]
    return " ".join(normalized)

# Fuzzy lookup
def fuzzy_matches(query: str, choices: list, limit=5, cutoff=75):
    hits = process.extract(
        query,
        choices,
        limit=limit,
        score_cutoff=cutoff
    )
    return [hit[0] for hit in hits]

# Core lookup

def get_plan_value(raw_condition: str, plan_id: str):
    # 1) preprocess + concept normalize
    step1 = preprocess(raw_condition)
    query = normalize_concepts(step1)
    glog.info("Query after NLP preprocessing: %s", query)

    if 'Data Point Name' not in DF.columns or plan_id not in DF.columns:
        return 500, f"CSV missing required columns or plan '{plan_id}'"

    # Exact match on normalized_name
    matches = DF[DF['normalized_name'] == query]
    # Fuzzy fallback
    if matches.empty:
        choices = DF['normalized_name'].tolist()
        close = fuzzy_matches(query, choices)
        if close:
            matches = DF[DF['normalized_name'].isin(close)]

    if matches.empty:
        return 404, f"No data-points matching '{raw_condition}' found"

    # Gather results
    results = []
    for _, row in matches.iterrows():
        val = row.get(plan_id)
        if pd.notna(val):
            results.append({
                "condition": row['Data Point Name'],
                "plan": plan_id,
                "value": val
            })

    if not results:
        return 404, f"No value for '{raw_condition}' under plan '{plan_id}'"
    return 200, results

# (rest of your Lambda handler and LLM summarization unchanged)

def wrap_response(status, body):
    return {
        "statusCode": status,
        "headers": {
            "Content-Type": "application/json",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "OPTIONS,POST"
        },
        "body": json.dumps(body)
    }

def invoke_fallback(event_payload):
    try:
        resp = _lambda.invoke(
            FunctionName=FALLBACK_LAMBDA_NAME,
            InvocationType="RequestResponse",
            Payload=json.dumps(event_payload)
        )
        return wrap_response(200, json.loads(resp['Payload'].read()))
    except Exception as e:
        glog.exception("Fallback invocation failed")
        return wrap_response(500, {"error": f"Fallback error: {e}"})

# Simplified handler (illustrative)
def lambda_handler(event, _context):
    body = json.loads(event.get('body', '{}'))
    condition = body.get('parameters', {}).get('condition')
    plan_id   = body.get('parameters', {}).get('plan')

    status, data = get_plan_value(condition, plan_id)
    if status != 200:
        return invoke_fallback({
            "parameters": [
                {"name": "condition", "value": condition},
                {"name": "plan",      "value": plan_id}
            ]
        })
    return wrap_response(200, {"results": data})
