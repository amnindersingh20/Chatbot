import logging
import boto3
from langchain_aws import ChatBedrock
from langchain_aws.retrievers import AmazonKnowledgeBasesRetriever
from langchain.chains import ConversationalRetrievalChain
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# â€” Logging setup â€”
logging.basicConfig(level=logging.INFO)

# â€” AWS / Bedrock client setup â€”
session = boto3.Session(profile_name="Amder")
llm = ChatBedrock(
    model_id="anthropic.claude-3-5-sonnet-2020-v1:0",
    region_name="us-east-1",
    client=session.client("bedrock-runtime", region_name="us-east-1")
)

retriever = AmazonKnowledgeBasesRetriever(
    knowledge_base_id="arn:aws:bedrock:us-east-1:123456789012:knowledge-base/my-kb-id",
    retrieval_config={
        "vectorSearchConfiguration": {
            "numberOfResults": 5,
            "overrideSearchType": "HYBRID"
        }
    },
    client=session.client("bedrock-agent-runtime", region_name="us-east-1")
)

# â€” Build the conversational QA chain with NO memory attached â€”
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    return_source_documents=True,
    return_generated_question=True,
    output_key="answer",
    verbose=False
)

# â€” In-memory history store for Runnable â€”
chat_history_memory = InMemoryChatMessageHistory()

# â€” Wrap chain so it automatically reads & writes that history â€”
runnable = RunnableWithMessageHistory(
    qa_chain,
    lambda _: chat_history_memory,
    input_messages_key="question",
    history_messages_key="chat_history"
)

session_id = "my-session"

if __name__ == "__main__":
    print("Type â€˜resetâ€™ to clear context, â€˜exitâ€™ to quit.")
    while True:
        user_input = input("You: ").strip()
        if user_input.lower() in {"exit", "quit"}:
            print("ğŸ‘‹ Goodbye!")
            break
        if user_input.lower() == "reset":
            chat_history_memory.clear()
            print("ğŸ§¹ Context reset!")
            continue

        # Always invoke via the Runnable!
        result = runnable.invoke(
            {"question": user_input},
            config={"configurable": {"session_id": session_id}}
        )

        # Show how it rewrote (if at all)
        rewritten = result.get("question")
        if rewritten and rewritten != user_input:
            print("ğŸ”„ Rewritten with context:", rewritten)
        else:
            print("ğŸ‘‰ Treated as fresh question")

        # Show whether KB docs were pulled in
        docs = result.get("source_documents", [])
        if docs:
            print(f"ğŸ“š Retrieved {len(docs)} doc(s); snippet:")
            print(docs[0].page_content[:200], "â€¦")
        else:
            print("ğŸ¤– No KB retrieval â€“ pure LLM response")

        # The actual answer
        print("\nğŸ’¬ Claude:", result["answer"])

        # Print the accumulated chat history
        print("\nğŸ“ Conversation history so far:")
        for msg in chat_history_memory.messages:
            print(f"{msg.type.capitalize()}: {msg.content}")
        print("\n" + "-"*60 + "\n")
