import logging
import boto3
from langchain_aws import ChatBedrock
from langchain_aws.retrievers import AmazonKnowledgeBasesRetriever
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# ‚Äî Logging (optional) ‚Äî
logging.basicConfig(level=logging.INFO)

# ‚Äî Boto3 session & Claude LLM client ‚Äî
session = boto3.Session(profile_name="Amder")
llm = ChatBedrock(
    model_id="anthropic.claude-3-5-sonnet-2020-v1:0",
    region_name="us-east-1",
    client=session.client("bedrock-runtime", region_name="us-east-1")
)

# ‚Äî Retriever pointing at your knowledge base ‚Äî
retriever = AmazonKnowledgeBasesRetriever(
    knowledge_base_id="TGMNY",
    retrieval_config={
        "vectorSearchConfiguration": {
            "numberOfResults": 5,
            "overrideSearchType": "HYBRID"
        }
    },
    client=session.client("bedrock-agent-runtime", region_name="us-east-1")
)

# ‚Äî Memory that the chain can use internally ‚Äî
buffer_memory = ConversationBufferMemory(llm=llm, max_token_limit=512)

# ‚Äî Build the conversational QA chain with memory attached ‚Äî
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=buffer_memory,           # ‚Üê here‚Äôs the magic
    return_source_documents=True,
    verbose=True
)

# ‚Äî In-process chat history store ‚Äî
chat_history_memory = InMemoryChatMessageHistory()

# ‚Äî Wrap in Runnable so history is auto-fed in & out ‚Äî
runnable = RunnableWithMessageHistory(
    qa_chain,
    lambda _: chat_history_memory,
    input_messages_key="question",
    history_messages_key="chat_history"
)

session_id = "my-session"

# ‚Äî Simple REPL loop ‚Äî
if __name__ == "__main__":
    print("Type ‚Äòreset‚Äô to clear context, ‚Äòexit‚Äô to quit.")
    while True:
        user_input = input("You: ").strip()
        if user_input.lower() in {"exit", "quit"}:
            break
        if user_input.lower() == "reset":
            chat_history_memory.clear()
            buffer_memory.clear()
            print("üßπ Context reset!")
            continue

        # Invoke via runnable‚Äîpulls in history, appends new turn
        result = runnable.invoke(
            {"question": user_input},
            config={"configurable": {"session_id": session_id}}
        )
        answer = result["answer"]
        print("Claude:", answer)

        # Show running history
        print("\nüìù Conversation History:")
        for msg in chat_history_memory.messages:
            print(f"{msg.type.capitalize()}: {msg.content}")
        print("\n" + "-"*40 + "\n")
